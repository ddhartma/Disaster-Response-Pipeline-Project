{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+http://github.com/scikit-learn/scikit-learn.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hartmann\\Miniconda3\\envs\\ds_ndp\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\cloudpickle\\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "C:\\Users\\hartmann\\Miniconda3\\envs\\ds_ndp\\lib\\site-packages\\sklearn\\utils\\validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n",
      "C:\\Users\\hartmann\\Miniconda3\\envs\\ds_ndp\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\hartmann\\Miniconda3\\envs\\ds_ndp\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\hartmann\\Miniconda3\\envs\\ds_ndp\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\hartmann\\Miniconda3\\envs\\ds_ndp\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\hartmann\\Miniconda3\\envs\\ds_ndp\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\hartmann\\Miniconda3\\envs\\ds_ndp\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1095: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\hartmann\\Miniconda3\\envs\\ds_ndp\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1340: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\hartmann\\Miniconda3\\envs\\ds_ndp\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1476: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "C:\\Users\\hartmann\\Miniconda3\\envs\\ds_ndp\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\hartmann\\Miniconda3\\envs\\ds_ndp\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "C:\\Users\\hartmann\\Miniconda3\\envs\\ds_ndp\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n",
      "C:\\Users\\hartmann\\Miniconda3\\envs\\ds_ndp\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py:34: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ._gradient_boosting import predict_stages\n",
      "C:\\Users\\hartmann\\Miniconda3\\envs\\ds_ndp\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py:34: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ._gradient_boosting import predict_stages\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import os\n",
    "from time import time\n",
    "import copy\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "#matplotlib.use('nbagg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import Markdown, display, HTML\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "import pickle\n",
    "\n",
    "# Needed for NLP processing\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger', 'stopwords'])\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# import moduls for ML LEarning\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from pipelinehelper import PipelineHelper\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "def load_data(database_filepath):\n",
    "    \"\"\" Load data from Sqlite database into DataFrame\n",
    "        INPUTS:\n",
    "        ------------\n",
    "            database_filepath (string) -  path to Sqlite database\n",
    "        \n",
    "        OUTPUTS:\n",
    "        ------------\n",
    "            X (numpy arrays) - input features (messages) of DataFrame df\n",
    "            Y (pandas DataFrame) - categories of DataFrame df\n",
    "            category_names (list) of category names (column names of Y)\n",
    "        \n",
    "    \"\"\"\n",
    "    engine = create_engine('sqlite:///'+ database_filepath)\n",
    "    df = pd.read_sql(\"SELECT * FROM disaster\", engine)\n",
    "    \n",
    "    # X and y\n",
    "    colnames = df.columns.tolist()\n",
    "    category_names = colnames[4:]\n",
    "\n",
    "    X = df.message.values\n",
    "    Y = df[category_names]\n",
    "    \n",
    "    # Show DataFrame result, shapes, colnames, target\n",
    "    print('DATAFRAME df')\n",
    "    display(df.head())\n",
    "    print('Shape of df: ' + str(df.shape))\n",
    "    print(' ')\n",
    "   \n",
    "    print('DATAFRAME Y')\n",
    "    display(Y.head())\n",
    "    print('Shape of Y: ' + str(Y.shape))\n",
    "    print(' ')\n",
    "    \n",
    "    print('colnames')\n",
    "    print(colnames)\n",
    "    print(' ')\n",
    "    \n",
    "    print('category_names')\n",
    "    print(category_names)\n",
    "    print(' ')\n",
    "    \n",
    "    return X, Y, category_names, df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, word_prep='lemmatize'):\n",
    "    \"\"\" function that will \n",
    "        - replace urls with spaceholder\n",
    "        - remove punctuation\n",
    "        - remove stopwords\n",
    "        - stem/lemmatize words \n",
    "        - normalize all words to lower case \n",
    "        - remove white spaces\n",
    "         \n",
    "        INPUTS:\n",
    "        ------------\n",
    "            text (string) - message text\n",
    "            \n",
    "        OUTPUTS:\n",
    "        ------------\n",
    "            clean_tokens (list) - of cleaned words\n",
    "    \n",
    "    \"\"\"\n",
    "    # Detect URLs\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')]  \n",
    "    \n",
    "    # Stem, normalize all words to lower case, remove white spaces \n",
    "    if word_prep == 'stem':\n",
    "        clean_tokens = [PorterStemmer().stem(tok).lower().strip() for tok in tokens]\n",
    "    \n",
    "    # Lemmatize, normalize all words to lower case, remove white spaces \n",
    "    if word_prep == 'lemmatize':\n",
    "        clean_tokens = [WordNetLemmatizer().lemmatize(tok).lower().strip() for tok in tokens]\n",
    "          \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Custom Transformer class that will \n",
    "        - replace urls with spaceholder\n",
    "        - remove punctuation\n",
    "        - remove stopwords\n",
    "        - stem/lemmatize words \n",
    "        - normalize all words to lower case \n",
    "        - remove white spaces\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, word_prep):\n",
    "        \"\"\" Init of CustomTokenizer\n",
    "        \n",
    "            INPUTS:\n",
    "            ------------\n",
    "                word_prep (string) - 'stem' or 'lemmatize',  to choose between stemming or lemmatization during tokenization\n",
    "                                     Useful for GridSearchCV \n",
    "                        \n",
    "\n",
    "            OUTPUTS:\n",
    "            ------------\n",
    "                no direct\n",
    "        \"\"\"\n",
    "        self.word_prep = word_prep\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        \"\"\" Function that will \n",
    "            - replace urls with placeholder\n",
    "            - remove punctuation\n",
    "            - remove stopwords\n",
    "            - stem/lemmatize words \n",
    "            - normalize all words to lower case \n",
    "            - remove white spaces\n",
    "         \n",
    "        INPUTS:\n",
    "        ------------\n",
    "            text (string) - message text\n",
    "        \n",
    "        OUTPUTS:\n",
    "        ------------\n",
    "            clean_tokens - a list of cleaned words\n",
    "    \n",
    "        \"\"\"\n",
    "        # Detect URLs\n",
    "        url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "        detected_urls = re.findall(url_regex, text)\n",
    "        for url in detected_urls:\n",
    "            text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords\n",
    "        tokens = [t for t in tokens if t not in stopwords.words('english')]  \n",
    "        \n",
    "        # Stem, normalize all words to lower case, remove white spaces \n",
    "        if self.word_prep == 'stem':\n",
    "            clean_tokens = [str(PorterStemmer().stem(tok)).lower().strip() for tok in tokens]\n",
    "\n",
    "        # Lemmatize, normalize all words to lower case, remove white spaces \n",
    "        if self.word_prep == 'lemmatize':\n",
    "            clean_tokens = [str(WordNetLemmatizer().lemmatize(tok)).lower().strip() for tok in tokens]\n",
    "\n",
    "        return clean_tokens\n",
    "    \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\" fit function for estimator (object that learns from data), \n",
    "            \n",
    "            INPUTS:\n",
    "            ------------\n",
    "                X (numpy.ndarray) - of training (testing) features  \n",
    "                y (numpy.ndarray) - with dataset target labels\n",
    "            \n",
    "            OUTPUTS\n",
    "            ------------\n",
    "                self - allows to chain methods together. This method is required to be compatible with scikit-learn\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\" function which includes the code to transform the data\n",
    "            \n",
    "            INPUTS:\n",
    "            ------------\n",
    "                X (numpy.ndarray) - of training (testing) features  \n",
    "            \n",
    "            OUTPUTS:\n",
    "            ------------\n",
    "                df_x_tagged (pandas DataFrame) - with a list of cleaned words in each row \n",
    "        \"\"\"\n",
    "        X_tagged = pd.Series(X).apply(self.tokenize)\n",
    "        df_x_tagged = pd.DataFrame(X_tagged)\n",
    "        print(df_x_tagged)\n",
    "        \n",
    "        return df_x_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a custom vectorizer class\n",
    "class CustomVectorizer(CountVectorizer): \n",
    "    \"\"\" A CustomVectorizer class which inherits from the CountVectorizer class. \n",
    "        Aim: switch between Porterstemmer and Lemmatization during training via GridSearchCV. \n",
    "        A CountVectorizer object converts a collection of text documents to a matrix of token counts.  \n",
    "    \"\"\"\n",
    "    def __init__(self, X, word_prep='lemmatize', remove_stopwords=True, **kwargs):\n",
    "        \"\"\" Init function that takes all arguments of CountVectorizer base class and adds two own arguments\n",
    "            \n",
    "            INPUTS:\n",
    "            ------------\n",
    "                X (numpy.ndarray) - of training (testing) features  \n",
    "                word_prep (string) - 'stem' or 'lemmatize',  to choose between stemming or lemmatization during tokenization\n",
    "                                     Useful for GridSearchCV \n",
    "            \n",
    "            OUTPUTS:\n",
    "            ------------\n",
    "                no direct outputs\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.X = X\n",
    "        self.word_prep = word_prep\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lowercase=False\n",
    "        \n",
    "\n",
    "    def prepare_doc(self, text):\n",
    "        #print(self.word_prep)\n",
    "        \"\"\" Function that will \n",
    "            - replace urls with spaceholder\n",
    "            - remove punctuation\n",
    "            - remove stopwords\n",
    "            - stem/lemmatize words \n",
    "            - normalize all words to lower case \n",
    "            - remove white spaces\n",
    "\n",
    "            INPUTS:\n",
    "            ------------\n",
    "                text (string) - message text\n",
    "\n",
    "            OUTPUTS:\n",
    "            ------------\n",
    "                clean_tokens (list) - of cleaned words\n",
    "    \n",
    "        \"\"\"\n",
    "    \n",
    "        # Detect URLs\n",
    "        url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "        detected_urls = re.findall(url_regex, text)\n",
    "        for url in detected_urls:\n",
    "            text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        if self.remove_stopwords == True:\n",
    "            tokens = [t for t in tokens if t not in stopwords.words('english')]  \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # Stem, normalize all words to lower case, remove white spaces \n",
    "        if self.word_prep == 'stem':\n",
    "            clean_tokens = [PorterStemmer().stem(tok).lower().strip() for tok in tokens]\n",
    "\n",
    "        # Lemmatize, normalize all words to lower case, remove white spaces \n",
    "        if self.word_prep == 'lemmatize':\n",
    "            clean_tokens = [WordNetLemmatizer().lemmatize(tok).lower().strip() for tok in tokens]\n",
    "        #print(clean_tokens)\n",
    "        return clean_tokens\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\" Overwrite get_params in CountVectorizer base class \n",
    "            create new get_params() including the new property word_prep\n",
    "            \n",
    "            INPUTS:\n",
    "            ------------\n",
    "                deep - parameter in get_params function of base class\n",
    "            \n",
    "            OUTPUTS:\n",
    "            ------------\n",
    "                params (dictionary) - new parameter dictionary\n",
    "        \"\"\"\n",
    "        params = super().get_params(deep)\n",
    "        \n",
    "        # Hack to make get_params return base class params...\n",
    "        cp = copy.copy(self)\n",
    "        cp.__class__ = CountVectorizer\n",
    "        \n",
    "        params.update(CountVectorizer.get_params(cp, deep))\n",
    "        return params\n",
    "        \n",
    "    def build_analyzer(self):\n",
    "        \"\"\" Overwrite build_analyzer in CountVectorizer base class \n",
    "            \n",
    "            INPUTS:\n",
    "            ------------\n",
    "            \n",
    "            OUTPUTS:\n",
    "            ------------\n",
    "                call prepare_doc, transform training (testing) features, \n",
    "                return cleaned lists of word tokenized messages\n",
    "        \"\"\"\n",
    "        preprocess = self.build_preprocessor()\n",
    "        return lambda doc : preprocess(self.decode(self.prepare_doc(doc)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfid_transform(X):\n",
    "    \"\"\" Get weighted idfs for each word of the Bag-of-Words (CountVectorizer matrix) \n",
    "        \n",
    "        INPUTS:\n",
    "        ------------\n",
    "            X (numpy.ndarray) - of training (testing) features \n",
    "        \n",
    "        OUTPUTS:\n",
    "        ------------\n",
    "            df_idf.sort_values(by=['idf_weights']) (pandas DataFrame) -  with all words from Bag-of-Words as index \n",
    "                                                                         and idf_weights as one column\n",
    "    \"\"\"\n",
    "    # Build the pipeline\n",
    "    pipeline = Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "            ])\n",
    "    \n",
    "    print('Compute the IDF values ...')\n",
    "    pipeline.fit(X)\n",
    "    \n",
    "    # create df_idf DataFrame\n",
    "    df_idf = pd.DataFrame(pipeline['tfidf'].idf_, index=pipeline['vect'].get_feature_names(),columns=[\"idf_weights\"]) \n",
    "    \n",
    "    # sort ascending \n",
    "    return df_idf.sort_values(by=['idf_weights'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Custom Transformer classes\n",
    "Let's create Custom Transformer classes to implement special NLP pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" To know the parts of speech (like nouns, verbs, pronouns) \n",
    "        can help to understand the meaning of a sentence better.\n",
    "        This class checks if the first word of a sentence is a verb.\n",
    "        if yes --> return True\n",
    "        if no --> return False\n",
    "    \n",
    "    \"\"\"\n",
    "    def starting_verb(self, text):\n",
    "        \"\"\" function that \n",
    "            - divides a text string into a list of sentences \n",
    "            - checks if the first word of a sentence is a verb\n",
    "            \n",
    "            INPUTS: \n",
    "            ------------\n",
    "            text - a string of text\n",
    "            \n",
    "            OUTPUTS:\n",
    "            ------------\n",
    "            True - if verb\n",
    "            False - if anything else than verb\n",
    "        \"\"\"\n",
    "            \n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        \"\"\" fit function for estimator (object that learns from data), \n",
    "            here estimator is an instance of StartingVerbExtractor class\n",
    "            \n",
    "            INPUTS:\n",
    "            ------------\n",
    "            x - 2d array X of the dataset features  \n",
    "            y - 1d array y of the dataset target labels\n",
    "            \n",
    "            OUTPUTS\n",
    "            ------------\n",
    "            self - allows to chain methods together. This method is required to be compatible with scikit-learn\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\" function which includes the code to transform the data\n",
    "            \n",
    "            INPUTS:\n",
    "            ------------\n",
    "            x - 2d array X of the dataset features\n",
    "            \n",
    "            OUTPUTS:\n",
    "            ------------\n",
    "            df_x_tagged - a DataFrame of X_tagged (containing a column with True and False values) \n",
    "                          this transformer object will be appendend to the pipeline object via Feature Union\n",
    "        \"\"\"\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        df_x_tagged = pd.DataFrame(X_tagged)\n",
    "        return df_x_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables.\n",
    "\n",
    "In order to successfully predict classification results for the 36 categories a pipeline helps to organize data preporcessing and data flow based on ETL and NLP.\n",
    "The used pipeline is structured in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X_train, Y_train, pipeline_name='pipeline_1'):\n",
    "    \"\"\" Build a ML pipelines\n",
    "        Test different pipelines\n",
    "        - pipeline_1: standard based on CountVectorizer, TfidfTransformer and MultiOutputClassifier\n",
    "        - pipeline_2: as pipeline_1 but with a CustomVectorizer and GridSearchCV to find optimized parameters\n",
    "        - pipeline_3: add the CustomTransformer 'StartingVerbExtractor' into pipeline via Feature Union\n",
    "        \n",
    "        INPUTS:\n",
    "        ------------\n",
    "        pipeline_name - string name for calling a specific pipeline\n",
    "        X_train - numpy.ndarray, input features for training \n",
    "        Y_train - numpy.ndarray, target values \n",
    "        \n",
    "        OUTPUTS:\n",
    "        ------------\n",
    "        cv  - model based on sklearn GridSearchCV and actual parameter settings\n",
    "        pipeline - model based on sklearn Pipeline (including ETL and NLP processing steps) \n",
    "        parameters - dictionary of GridSearchCV parameters\n",
    "        \n",
    "    \"\"\"\n",
    "    if pipeline_name == 'pipeline_1':\n",
    "        print('pipeline_1 chosen')\n",
    "        pipeline = Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', MultiOutputClassifier(RandomForestClassifier())),\n",
    "                #('clf', SGDClassifier()),\n",
    "            ])\n",
    "        return pipeline, None, None\n",
    "    \n",
    "    \n",
    "    if pipeline_name == 'pipeline_2':\n",
    "        print('pipeline_2 chosen')\n",
    "        pipeline = Pipeline([\n",
    "            ('nlp', Pipeline([\n",
    "                #('tokenizer', CustomTokenizer(word_prep='stem')),\n",
    "                ('vect', CustomVectorizer(X_train, word_prep='lemmatize')),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "            ])),\n",
    "            ('classifier', PipelineHelper([\n",
    "                ('rfc', MultiOutputClassifier(RandomForestClassifier())),\n",
    "                ('abc', MultiOutputClassifier(\n",
    "                        AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1, class_weight='balanced'))))\n",
    "            ]))\n",
    "        ])\n",
    "       \n",
    "        # uncommenting more parameters will give better exploring power but will\n",
    "        # increase processing time in a combinatorial way\n",
    "        parameters = {\n",
    "                #'nlp__vect__word_prep': ('stem', 'lemmatize'),\n",
    "                'nlp__vect__remove_stopwords': (True, 'False'),\n",
    "                #'nlp__vect__max_df': (0.5, 0.75, 1.0),\n",
    "                #'nlp__vect__max_features': (None, 5000, 10000, 50000),\n",
    "                #'nlp__vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "                #'nlp__tfidf__use_idf': (True, False),\n",
    "                #'nlp__tfidf__norm': ('l1', 'l2'),\n",
    "            \n",
    "            'classifier__selected_model': pipeline.named_steps['classifier'].generate({\n",
    "                #'rfc__estimator__n_estimators': [10, 20],\n",
    "                #'rfc__estimator__min_samples_split': [2, 5]\n",
    "                'abc__estimator__learning_rate': [0.1, 0.3],\n",
    "                'abc__estimator__n_estimators': [100, 200],\n",
    "            })\n",
    "        }\n",
    "        cv = GridSearchCV(estimator=pipeline, param_grid=parameters, n_jobs=1, verbose=1)\n",
    "        #cv.fit(X_train, Y_train)\n",
    "        return cv, pipeline, parameters\n",
    "\n",
    "        \n",
    "    if pipeline_name == 'pipeline_3':\n",
    "        print('pipeline_3 chosen')\n",
    "        pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('starting_verb', StartingVerbExtractor())\n",
    "        ])),\n",
    "\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "        ])\n",
    "        parameters = {\n",
    "            #'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "            #'features__text_pipeline__vect__max_df': (0.5, 0.75, 1.0),\n",
    "            'features__text_pipeline__vect__max_features': (None, 5000, 10000),\n",
    "            'features__text_pipeline__tfidf__use_idf': (True, False),\n",
    "            #'clf__estimator__n_estimators': [50, 100, 200],\n",
    "            #'clf__estimator__min_samples_split': [2, 3, 4],\n",
    "            'features__transformer_weights': (\n",
    "                {'text_pipeline': 1, 'starting_verb': 0.5},\n",
    "                {'text_pipeline': 0.5, 'starting_verb': 1},\n",
    "                #{'text_pipeline': 0.8, 'starting_verb': 1},\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        cv = GridSearchCV(estimator=pipeline, param_grid=parameters, n_jobs=1, verbose=1)\n",
    "        #cv.fit(X_train, Y_train)\n",
    "        return cv, pipeline, parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, df, X_train, X_test, Y_test, category_names, pipeline_name):\n",
    "    \"\"\" Get a classification report from testing results including precision, recall, f1-score an accuracy \n",
    "        \n",
    "        INPUTS:\n",
    "        ------------\n",
    "            model (sklearn object) - the trained model based on the actual pipeline\n",
    "            X_test (pandas DataFrame) - with test input features \n",
    "            Y_test (pandas DataFrame) - of true target values \n",
    "        \n",
    "        OUTPUTS:\n",
    "        ------------\n",
    "            print statements for precision, recall, f1-score an accuracy for each category\n",
    "            print statement for size of Bag-of-Words\n",
    "            print statements and csv export for message stats\n",
    "            print statements and csv export for 20 most common words\n",
    "            print statements and csv export for 100 randomly chosen messages (in raw format and tokenized)\n",
    "        \n",
    "    \"\"\"\n",
    "    # Make predictions based on trained model\n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    #print(classification_report(y_test, y_pred, target_names=y_test.keys()))\n",
    "    accuracy = (Y_pred == Y_test).mean()\n",
    "    df_classification_report = pd.DataFrame(classification_report(Y_test, Y_pred, target_names=Y_test.keys(),  output_dict=True))\n",
    "    df_classification_report = pd.concat([df_classification_report.T, accuracy], axis=1).reindex(df_classification_report.T.index)\n",
    "    df_classification_report.columns = ['f1_score', 'precision', 'recall', 'support', 'accuracy']\n",
    "    print(pipeline_name)\n",
    "    print(df_classification_report)\n",
    "    print(' ')\n",
    "    print('Total accuracy = ' + str(round(accuracy.mean(),2)))\n",
    "    print(' ')\n",
    "    \n",
    "    # get most commomn words\n",
    "    most_comon_words = tfid_transform(X_train)\n",
    "    most_comon_words.to_csv('most_common_words.csv')\n",
    "    print('20 most common words')\n",
    "    print(list(most_comon_words.index)[:20])\n",
    "    print(' ')\n",
    "    print('Size of Bag-of-Words: ', len(list(most_comon_words.index)))\n",
    "    print(' ')\n",
    "    print('... most_common_words.csv saved!')\n",
    "    print(' ')\n",
    "    \n",
    "    # Check 100 randomly chosen messages --- coming from X_train with and without tokenization\n",
    "    rand_set = np.random.randint(df.shape[0], size=(1, 100))\n",
    "    message_raw = []\n",
    "    message_tok = []\n",
    "    for index in rand_set[0]:\n",
    "        try:\n",
    "            print(X_train[index])\n",
    "            print('')\n",
    "            print(tokenize(X_train[index]))\n",
    "            print('--------------------------------------------------------')\n",
    "            message_raw.append(X_train[index])\n",
    "            message_tok.append(tokenize(X_train[index]))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    message_set = pd.DataFrame({'message_raw': message_raw, 'message_tok': message_tok})\n",
    "    message_set.to_csv('message_set.csv')\n",
    "    print('... message_set.csv saved!')\n",
    "    print(' ')\n",
    "    \n",
    "    # distribution of word counts for each genre\n",
    "    # create boxplot and Histograms: What is the distribution of word-count for each genre? Are there any outliers?\n",
    "    print('Tokenize messages ...')\n",
    "    message_stats_direct = df[df['genre'] == 'direct']['message'].apply(lambda x: len(tokenize(x)))\n",
    "    message_stats_news = df[df['genre'] == 'news']['message'].apply(lambda x: len(tokenize(x)))\n",
    "    message_stats_social = df[df['genre'] == 'social']['message'].apply(lambda x: len(tokenize(x)))\n",
    "    print('Median of direct message word count: ', message_stats_direct.median())\n",
    "    print('Median of news message word count: ', message_stats_news.median())\n",
    "    print('Median of social message word count: ', message_stats_social.median())\n",
    "    print(' ')\n",
    "    message_stats_direct.to_csv('message_stats_direct.csv')\n",
    "    print('... message_stats_direct.csv saved!')\n",
    "    message_stats_news.to_csv('message_stats_news.csv')\n",
    "    print('... message_stats_news.csv saved!')\n",
    "    message_stats_social.to_csv('message_stats_social.csv')\n",
    "    print('... message_stats_social.csv saved!')\n",
    "        \n",
    "    print('... messages_stats.json saved!')\n",
    "    print(' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_filepath):\n",
    "    \"\"\" Save the model\n",
    "        \n",
    "        INPUTS:\n",
    "        ------------\n",
    "        model: model to be saved\n",
    "        model_filepath: filepath to model\n",
    "        \n",
    "        OUTPUTS:\n",
    "        ------------\n",
    "        save model as a pickle file\n",
    "    \"\"\"  \n",
    "    pickle.dump(model, open(model_filepath, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "    DATABASE: disaster.db\n",
      "DATAFRAME df\n",
      "   id                                            message  \\\n",
      "0   2  Weather update - a cold front from Cuba that c...   \n",
      "1   7            Is the Hurricane over or is it not over   \n",
      "2   8                    Looking for someone but no name   \n",
      "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
      "4  12  says: west side of Haiti, rest of the country ...   \n",
      "\n",
      "                                            original   genre  related  \\\n",
      "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
      "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
      "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
      "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
      "4  facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
      "\n",
      "   request  offer  aid_related  medical_help  medical_products  ...  \\\n",
      "0        0      0            0             0                 0  ...   \n",
      "1        0      0            1             0                 0  ...   \n",
      "2        0      0            0             0                 0  ...   \n",
      "3        1      0            1             0                 1  ...   \n",
      "4        0      0            0             0                 0  ...   \n",
      "\n",
      "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
      "0            0                     0                0       0      0     0   \n",
      "1            0                     0                1       0      1     0   \n",
      "2            0                     0                0       0      0     0   \n",
      "3            0                     0                0       0      0     0   \n",
      "4            0                     0                0       0      0     0   \n",
      "\n",
      "   earthquake  cold  other_weather  direct_report  \n",
      "0           0     0              0              0  \n",
      "1           0     0              0              0  \n",
      "2           0     0              0              0  \n",
      "3           0     0              0              0  \n",
      "4           0     0              0              0  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "Shape of df: (26028, 40)\n",
      " \n",
      "DATAFRAME Y\n",
      "   related  request  offer  aid_related  medical_help  medical_products  \\\n",
      "0        1        0      0            0             0                 0   \n",
      "1        1        0      0            1             0                 0   \n",
      "2        1        0      0            0             0                 0   \n",
      "3        1        1      0            1             0                 1   \n",
      "4        1        0      0            0             0                 0   \n",
      "\n",
      "   search_and_rescue  security  military  child_alone  ...  aid_centers  \\\n",
      "0                  0         0         0            0  ...            0   \n",
      "1                  0         0         0            0  ...            0   \n",
      "2                  0         0         0            0  ...            0   \n",
      "3                  0         0         0            0  ...            0   \n",
      "4                  0         0         0            0  ...            0   \n",
      "\n",
      "   other_infrastructure  weather_related  floods  storm  fire  earthquake  \\\n",
      "0                     0                0       0      0     0           0   \n",
      "1                     0                1       0      1     0           0   \n",
      "2                     0                0       0      0     0           0   \n",
      "3                     0                0       0      0     0           0   \n",
      "4                     0                0       0      0     0           0   \n",
      "\n",
      "   cold  other_weather  direct_report  \n",
      "0     0              0              0  \n",
      "1     0              0              0  \n",
      "2     0              0              0  \n",
      "3     0              0              0  \n",
      "4     0              0              0  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "Shape of Y: (26028, 36)\n",
      " \n",
      "colnames\n",
      "['id', 'message', 'original', 'genre', 'related', 'request', 'offer', 'aid_related', 'medical_help', 'medical_products', 'search_and_rescue', 'security', 'military', 'child_alone', 'water', 'food', 'shelter', 'clothing', 'money', 'missing_people', 'refugees', 'death', 'other_aid', 'infrastructure_related', 'transport', 'buildings', 'electricity', 'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure', 'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold', 'other_weather', 'direct_report']\n",
      " \n",
      "category_names\n",
      "['related', 'request', 'offer', 'aid_related', 'medical_help', 'medical_products', 'search_and_rescue', 'security', 'military', 'child_alone', 'water', 'food', 'shelter', 'clothing', 'money', 'missing_people', 'refugees', 'death', 'other_aid', 'infrastructure_related', 'transport', 'buildings', 'electricity', 'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure', 'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold', 'other_weather', 'direct_report']\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24944\\1793812542.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Trained model saved!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m \u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pipeline_1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24944\\1793812542.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(pipeline_names)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpipeline_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Building model...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpipeline_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'pipeline_2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pipeline_3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "def start(pipeline_names):\n",
    "    \"\"\" Main function to trigger model training, model evaluation and model saving\n",
    "        Function that tiggers \n",
    "        - load_data\n",
    "        - train_test_split\n",
    "        - build_model\n",
    "        - model.fit \n",
    "        - evaluate_model\n",
    "        - save_model\n",
    "    \n",
    "        INPUTS:\n",
    "        ------------\n",
    "            pipeline_names (list) - of pipelines to execute, e.g. ['pipeline_2', 'pipeline_3']\n",
    "                                    These pipelines are called via the pipeline_names list \n",
    "        \n",
    "        OUTPUTS:\n",
    "        ------------\n",
    "            no odirect outputs, however the model is stored as a pickle file to disk\n",
    "        \n",
    "    \"\"\"\n",
    "   \n",
    "    database_filepath = 'disaster.db'\n",
    "    model_filepath = 'models/classifier.pkl'\n",
    "    print('Loading data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "    \n",
    "    # load data\n",
    "    X, Y, category_names, df = load_data(database_filepath)\n",
    "    \n",
    "    # train test split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    \n",
    "    # start pipelining, build the model\n",
    "    for pipeline_name in tqdm.tqdm(pipeline_names):\n",
    "        print('Building model...')\n",
    "        model, pipeline, parameters = build_model(X_train, Y_train, pipeline_name)\n",
    "        \n",
    "        if pipeline_name in ['pipeline_2', 'pipeline_3']:\n",
    "            print(\"Performing grid search...\")\n",
    "            print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "            print(\"parameters:\")\n",
    "            print(parameters)\n",
    "            \n",
    "            t0 = time()\n",
    "        \n",
    "        # train the model\n",
    "        print('Training model...')\n",
    "        model.fit(X_train, Y_train)\n",
    "        #print(pipeline['vect'].get_feature_names())\n",
    "        \n",
    "        if pipeline_name in ['pipeline_2', 'pipeline_3']:\n",
    "            print(\"done in %0.3fs\" % (time() - t0))\n",
    "            print()\n",
    "\n",
    "            print(\"Best score: %0.3f\" % model.best_score_)\n",
    "            print(\"Best parameters set:\")\n",
    "            best_parameters = model.best_estimator_.get_params()\n",
    "            for param_name in sorted(parameters.keys()):\n",
    "                print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "        print('Evaluating model...')\n",
    "        \n",
    "        # evaluate the model\n",
    "        evaluate_model(model, df, X_train, X_test, Y_test, category_names, pipeline_name)\n",
    "\n",
    "        # save the model\n",
    "        path, filename = os.path.split(model_filepath)\n",
    "        base, ext  = os.path.splitext(filename)\n",
    "        model_filepath = os.path.join(path, base + '_' + pipeline_name + '.pkl')\n",
    "        print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "        save_model(model, model_filepath)\n",
    "\n",
    "        print('Trained model saved!')\n",
    "\n",
    "start('pipeline_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --> see build_model --> pipeline_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --> see build_model --> pipeline_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --> see build_model --> pipeline_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see save_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last but very important part of a CRISP-DM analysis. In this section the following impoortant business questions will be answered:\n",
    "\n",
    "- Question 1: How are the three different 'genre' types distributed?\n",
    "- Question 2: What is the distribution of letters-count for each genre? Are there any outliers?\n",
    "- Question 3: What is the distribution of words-counts for each genre? Are there any outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_filepath = 'disaster.db'\n",
    "model_filepath = 'models/classifier.pkl'\n",
    "X, Y, category_names, df = load_data(database_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: How are the three different 'genre' types distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# as a bar chart    \n",
    "graph = []\n",
    "# extract data needed for visuals\n",
    "genre_counts = df.groupby('genre').count()['message']\n",
    "genre_names = list(genre_counts.index)\n",
    "\n",
    "graph.append(\n",
    "  go.Bar(\n",
    "  x = genre_names,\n",
    "  y = genre_counts,\n",
    "  )\n",
    ")\n",
    "\n",
    "layout = dict(title = 'Distribution of Message genres',\n",
    "            xaxis = dict(title = 'Count',),\n",
    "            yaxis = dict(title = 'Genre'),\n",
    "            )\n",
    "\n",
    "# append all charts to the figures list\n",
    "figures = []\n",
    "figures.append(dict(data=graph, layout=layout))\n",
    "#figures.append(dict(data=graph_two, layout=layout_two))\n",
    "\n",
    "fig = go.Figure(figures[0])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative to plotly: Values counts for type of message\n",
    "df['genre'].value_counts()\n",
    "sns.barplot(x=df['genre'].value_counts().index, y=df['genre'].value_counts());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: What is the distribution of letters-count for each genre? Are there any outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a Boxplot    \n",
    "\n",
    "graph = []\n",
    "# extract data needed for visuals\n",
    "message_length_direct = df[df['genre'] == 'direct']['message'].apply(lambda x: len(x))\n",
    "message_length_news = df[df['genre'] == 'news']['message'].apply(lambda x: len(x))\n",
    "message_length_social = df[df['genre'] == 'social']['message'].apply(lambda x: len(x))\n",
    "messages_box = [('direct', message_length_direct), ('news', message_length_news), ('social', message_length_social)]\n",
    "\n",
    "graph = [go.Box(y=message_length, name=message_type) for message_type, message_length in messages_box]\n",
    "    \n",
    "layout = dict(title = 'Letters-count descriptive stats for each genre')\n",
    "\n",
    "# append all charts to the figures list\n",
    "figures = []\n",
    "figures.append(dict(data=graph, layout=layout))\n",
    "\n",
    "fig = go.Figure(figures[0])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a Histogram    \n",
    "\n",
    "graph = []\n",
    "# extract data needed for visuals\n",
    "message_length_direct = df[df['genre'] == 'direct']['message'].apply(lambda x: len(x))\n",
    "message_length_news = df[df['genre'] == 'news']['message'].apply(lambda x: len(x))\n",
    "message_length_social = df[df['genre'] == 'social']['message'].apply(lambda x: len(x))\n",
    "messages_box = [('direct', message_length_direct), ('news', message_length_news), ('social', message_length_social)]\n",
    "\n",
    "graph = [go.Histogram(x=message_length, name=message_type) for message_type, message_length in messages_box]\n",
    "    \n",
    "layout = dict(title = 'Letters-count distribution for each genre',\n",
    "                    xaxis = dict(title = '# letters'),\n",
    "                    yaxis = dict(title = 'Count', type='log'),\n",
    "                    )\n",
    "\n",
    "# append all charts to the figures list\n",
    "figures = []\n",
    "figures.append(dict(data=graph, layout=layout))\n",
    "\n",
    "fig = go.Figure(figures[0])\n",
    "#fig.update_layout(yaxis_type=\"log\") \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: What is the distribution of words-counts for each genre? Are there any outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a Boxplot    \n",
    "\n",
    "graph = []\n",
    "# extract data needed for visuals\n",
    "\n",
    "messages_stats_direct = pd.read_csv(open('message_stats_direct.csv'))\n",
    "messages_stats_news = pd.read_csv(open('message_stats_news.csv'))\n",
    "messages_stats_social = pd.read_csv(open('message_stats_social.csv'))\n",
    "messages_stats = {'direct': messages_stats_direct, 'news': messages_stats_news, 'social': messages_stats_social}\n",
    "\n",
    "\n",
    "graph = [go.Box(y=message_length.iloc[:,1], name=message_type) for message_type, message_length in messages_stats.items()]\n",
    "    \n",
    "layout = dict(title = 'Word-count descriptive stats for each genre')\n",
    "\n",
    "# append all charts to the figures list\n",
    "figures = []\n",
    "figures.append(dict(data=graph, layout=layout))\n",
    "\n",
    "fig = go.Figure(figures[0])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a Histogram    \n",
    "\n",
    "graph = []\n",
    "# extract data needed for visuals\n",
    "# \n",
    "graph = [go.Histogram(x=message_length.iloc[:,1], name=message_type) for message_type, message_length in messages_stats.items()]\n",
    "    \n",
    "layout = dict(title = 'Word-count distribution for each genre',\n",
    "                    xaxis = dict(title = '# words'),\n",
    "                    yaxis = dict(title = 'Count', type='log'),\n",
    "                    )\n",
    "\n",
    "# append all charts to the figures list\n",
    "figures = []\n",
    "figures.append(dict(data=graph, layout=layout))\n",
    "\n",
    "fig = go.Figure(figures[0])\n",
    "#fig.update_layout(yaxis_type=\"log\") \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: What are the 20 most common words in the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset\n",
    "database_filepath = 'disaster.db'\n",
    "model_filepath = 'models/classifier.pkl'\n",
    "print('Loading data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "X, Y, category_names, df = load_data(database_filepath)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 100 randomly chosen messages --- coming from X_train with and without tokenization\n",
    "rand_set = np.random.randint(df.shape[0], size=(1, 100))\n",
    "rand_set\n",
    "message_raw = []\n",
    "message_tok = []\n",
    "for index in rand_set[0]:\n",
    "    try:\n",
    "        print(X_train[index])\n",
    "        print('')\n",
    "        print(tokenize(X_train[index]))\n",
    "        print('--------------------------------------------------------')\n",
    "        message_raw.append(X_train[index])\n",
    "        message_tok.append(tokenize(X_train[index]))\n",
    "    except:\n",
    "        pass\n",
    "message_set = pd.DataFrame(zip(message_raw,message_tok), columns=['message_raw', 'message_tok'])\n",
    "message_set.to_csv('message_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_comon_words = tfid_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_comon_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# as a bar chart    \n",
    "graph = []\n",
    "# extract data needed for visuals\n",
    "most_comon_words = pd.read_csv('most_common_words.csv', index_col=[0])\n",
    "print(most_comon_words.head())\n",
    "y = list(most_comon_words.index)[:20]\n",
    "print(y)\n",
    "x = most_comon_words['idf_weights']\n",
    "graph.append(\n",
    "  go.Bar(\n",
    "  x = x,\n",
    "  y = y,\n",
    "  orientation='h'\n",
    "  )\n",
    ")\n",
    "\n",
    "layout = dict(title = 'Most common words after tokenization',\n",
    "            xaxis = dict(title = 'Most common words',),\n",
    "            yaxis = dict(title = 'idf_weights'),\n",
    "            )\n",
    "\n",
    "# append all charts to the figures list\n",
    "figures = []\n",
    "figures.append(dict(data=graph, layout=layout))\n",
    "#figures.append(dict(data=graph_two, layout=layout_two))\n",
    "\n",
    "fig = go.Figure(figures[0])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result:\n",
    "Notice for words with lowest IDF values it is expected that these words appear more often. For idf_weights=1 they would appear in each and every document in our collection. The lower the IDF value of a word, the less unique it is to any particular document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5: Are there any significant correlations between the categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a bar Corrlation plot    \n",
    "\n",
    "graph = []\n",
    "# extract data needed for visuals\n",
    "corr_x = df.corr().index\n",
    "corr_y = df.corr().index\n",
    "\n",
    "graph.append(go.Heatmap(\n",
    "                    x=corr_x,\n",
    "                    y=corr_y,\n",
    "                    z=df.corr().values,\n",
    "                    type = 'heatmap',\n",
    "                    colorscale = 'Viridis')\n",
    "            )\n",
    "    \n",
    "layout = dict(title = 'Correlation matrix',\n",
    "                    xaxis = dict(automargin=True),\n",
    "                    yaxis = dict(automargin=True),\n",
    "             )\n",
    "\n",
    "# append all charts to the figures list\n",
    "figures = []\n",
    "figures.append(dict(data=graph, layout=layout))\n",
    "\n",
    "fig = go.Figure(figures[0])\n",
    "#fig.update_layout(yaxis_type=\"log\") \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "ds_ndp",
   "language": "python",
   "name": "ds_ndp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
